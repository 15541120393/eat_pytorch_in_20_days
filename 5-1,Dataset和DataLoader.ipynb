{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd775b13",
   "metadata": {},
   "source": [
    "# 5-1, Dataset和DataLoader\n",
    "\n",
    "Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道。\n",
    "\n",
    "Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。\n",
    "\n",
    "而DataLoader定义了按batch加载数据集的方法，它是一个实现了`__iter__`方法的可迭代对象，每次迭代输出一个batch的数据。\n",
    "\n",
    "DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。\n",
    "\n",
    "在绝大部分情况下，用户只需实现Dataset的`__len__`方法和`__getitem__`方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1565cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "\n",
    "print(\"torch.__version__=\"+torch.__version__) \n",
    "print(\"torchvision.__version__=\"+torchvision.__version__) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45557a5b",
   "metadata": {},
   "source": [
    "```\n",
    "torch.__version__=1.10.0\n",
    "torchvision.__version__=0.11.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7a37f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<font color=\"red\">\n",
    " \n",
    "公众号 **算法美食屋** 回复关键词：**pytorch**， 获取本项目源码和所用数据集百度云盘下载链接。\n",
    "    \n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875fb8c",
   "metadata": {},
   "source": [
    "### 一，Dataset和DataLoader概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f8516",
   "metadata": {},
   "source": [
    "**1，获取一个batch数据的步骤**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906005c3",
   "metadata": {},
   "source": [
    "让我们考虑一下从一个数据集中获取一个batch的数据需要哪些步骤。\n",
    "\n",
    "(假定数据集的特征和标签分别表示为张量`X`和`Y`，数据集可以表示为`(X,Y)`, 假定batch大小为`m`)\n",
    "\n",
    "1，首先我们要确定数据集的长度`n`。\n",
    "\n",
    "结果类似：`n = 1000`。\n",
    "\n",
    "2，然后我们从`0`到`n-1`的范围中抽样出`m`个数(batch大小)。\n",
    "\n",
    "假定`m=4`, 拿到的结果是一个列表，类似：`indices = [1,4,8,9]`\n",
    "\n",
    "3，接着我们从数据集中去取这`m`个数对应下标的元素。\n",
    "\n",
    "拿到的结果是一个元组列表，类似：`samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]`\n",
    "\n",
    "4，最后我们将结果整理成两个张量作为输出。\n",
    "\n",
    "拿到的结果是两个张量，类似`batch = (features,labels) `， \n",
    "\n",
    "其中 `features = torch.stack([X[1],X[4],X[8],X[9]])`\n",
    "\n",
    "`labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d2de58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c26c2d91",
   "metadata": {},
   "source": [
    "**2，Dataset和DataLoader的功能分工**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99820480",
   "metadata": {},
   "source": [
    "上述第1个步骤确定数据集的长度是由 Dataset的`__len__` 方法实现的。\n",
    "\n",
    "第2个步骤从`0`到`n-1`的范围中抽样出`m`个数的方法是由 DataLoader的 `sampler`和 `batch_sampler`参数指定的。\n",
    "\n",
    "`sampler`参数指定单个元素抽样方法，一般无需用户设置，程序默认在DataLoader的参数`shuffle=True`时采用随机抽样，`shuffle=False`时采用顺序抽样。\n",
    "\n",
    "`batch_sampler`参数将多个抽样的元素整理成一个列表，一般无需用户设置，默认方法在DataLoader的参数`drop_last=True`时会丢弃数据集最后一个长度不能被batch大小整除的批次，在`drop_last=False`时保留最后一个批次。\n",
    "\n",
    "第3个步骤的核心逻辑根据下标取数据集中的元素 是由 Dataset的 `__getitem__`方法实现的。\n",
    "\n",
    "第4个步骤的逻辑由DataLoader的参数`collate_fn`指定。一般情况下也无需用户设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0e25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7951b4",
   "metadata": {},
   "source": [
    "**3，Dataset和DataLoader的主要接口**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10273a5f",
   "metadata": {},
   "source": [
    "以下是 Dataset和 DataLoader的核心接口逻辑伪代码，不完全和源码一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self,dataset,batch_size,collate_fn,shuffle = True,drop_last = False):\n",
    "        self.dataset = dataset\n",
    "        self.collate_fn = collate_fn\n",
    "        self.sampler =torch.utils.data.RandomSampler if shuffle else \\\n",
    "           torch.utils.data.SequentialSampler\n",
    "        self.batch_sampler = torch.utils.data.BatchSampler\n",
    "        self.sample_iter = self.batch_sampler(\n",
    "            self.sampler(range(len(dataset))),\n",
    "            batch_size = batch_size,drop_last = drop_last)\n",
    "        \n",
    "    def __next__(self):\n",
    "        indices = next(self.sample_iter)\n",
    "        batch = self.collate_fn([self.dataset[i] for i in indices])\n",
    "        return batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006dbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a63a046b",
   "metadata": {},
   "source": [
    "### 二，使用Dataset创建数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c4e54",
   "metadata": {},
   "source": [
    "Dataset创建数据集常用的方法有：\n",
    "\n",
    "* 使用 torch.utils.data.TensorDataset 根据Tensor创建数据集(numpy的array，Pandas的DataFrame需要先转换成Tensor)。\n",
    "\n",
    "* 使用 torchvision.datasets.ImageFolder 根据图片目录创建图片数据集。\n",
    "\n",
    "* 继承 torch.utils.data.Dataset 创建自定义数据集。\n",
    "\n",
    "\n",
    "此外，还可以通过\n",
    "\n",
    "* torch.utils.data.random_split 将一个数据集分割成多份，常用于分割训练集，验证集和测试集。\n",
    "\n",
    "* 调用Dataset的加法运算符(`+`)将多个数据集合并成一个数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478dac7",
   "metadata": {},
   "source": [
    "**1，根据Tensor创建数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73763881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import TensorDataset,Dataset,DataLoader,random_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4aca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据Tensor创建数据集\n",
    "\n",
    "from sklearn import datasets \n",
    "iris = datasets.load_iris()\n",
    "ds_iris = TensorDataset(torch.tensor(iris.data),torch.tensor(iris.target))\n",
    "\n",
    "# 分割成训练集和预测集\n",
    "n_train = int(len(ds_iris)*0.8)\n",
    "n_val = len(ds_iris) - n_train\n",
    "ds_train,ds_val = random_split(ds_iris,[n_train,n_val])\n",
    "\n",
    "print(type(ds_iris))\n",
    "print(type(ds_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用DataLoader加载数据集\n",
    "dl_train,dl_val = DataLoader(ds_train,batch_size = 8),DataLoader(ds_val,batch_size = 8)\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    print(features,labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演示加法运算符（`+`）的合并作用\n",
    "\n",
    "ds_data = ds_train + ds_val\n",
    "\n",
    "print('len(ds_train) = ',len(ds_train))\n",
    "print('len(ds_valid) = ',len(ds_val))\n",
    "print('len(ds_train+ds_valid) = ',len(ds_data))\n",
    "\n",
    "print(type(ds_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c92621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03438ccf",
   "metadata": {},
   "source": [
    "**2，根据图片目录创建图片数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50c3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms,datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#演示一些常用的图片增强操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('./data/cat.jpeg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1bc7aa",
   "metadata": {},
   "source": [
    "![](./data/5-1-傻乎乎.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机数值翻转\n",
    "transforms.RandomVerticalFlip()(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facff25",
   "metadata": {},
   "source": [
    "![](./data/5-1-翻转.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cc9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机旋转\n",
    "transforms.RandomRotation(45)(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541a7b1",
   "metadata": {},
   "source": [
    "![](./data/5-1-旋转.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义图片增强操作\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "   transforms.RandomHorizontalFlip(), #随机水平翻转\n",
    "   transforms.RandomVerticalFlip(), #随机垂直翻转\n",
    "   transforms.RandomRotation(45),  #随机在45度角度内旋转\n",
    "   transforms.ToTensor() #转换成张量\n",
    "  ]\n",
    ") \n",
    "\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "  ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据图片目录创建数据集\n",
    "\n",
    "def transform_label(x):\n",
    "    return torch.tensor([x]).float()\n",
    "\n",
    "ds_train = datasets.ImageFolder(\"./eat_pytorch_datasets/cifar2/train/\",\n",
    "            transform = transform_train,target_transform= transform_label)\n",
    "ds_val = datasets.ImageFolder(\"./eat_pytorch_datasets/cifar2/test/\",\n",
    "\n",
    "\n",
    "print(ds_train.class_to_idx)\n",
    "\n",
    "# 使用DataLoader加载数据集\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = 50,shuffle = True)\n",
    "dl_val = DataLoader(ds_val,batch_size = 50,shuffle = True)\n",
    "\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd3a479",
   "metadata": {},
   "source": [
    "```\n",
    "{'0_airplane': 0, '1_automobile': 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a767e7",
   "metadata": {},
   "source": [
    "```\n",
    "torch.Size([50, 3, 32, 32])\n",
    "torch.Size([50, 1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb0e3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "562e9f0f",
   "metadata": {},
   "source": [
    "**3，创建自定义数据集**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d280d5c",
   "metadata": {},
   "source": [
    "下面通过继承Dataset类创建imdb文本分类任务的自定义数据集。\n",
    "\n",
    "大概思路如下：首先，对训练集文本分词构建词典。然后将训练集文本和测试集文本数据转换成token单词编码。\n",
    "\n",
    "接着将转换成单词编码的训练集数据和测试集数据按样本分割成多个文件，一个文件代表一个样本。\n",
    "\n",
    "最后，我们可以根据文件名列表获取对应序号的样本内容，从而构建Dataset数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3475c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import OrderedDict\n",
    "import re,string\n",
    "\n",
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20 \n",
    "\n",
    "\n",
    "train_data_path = 'eat_pytorch_datasets/imdb/train.tsv'\n",
    "test_data_path = 'eat_pytorch_datasets/imdb/test.tsv'\n",
    "train_token_path = 'eat_pytorch_datasets/imdb/train_token.tsv'\n",
    "test_token_path =  'eat_pytorch_datasets/imdb/test_token.tsv'\n",
    "train_samples_path = 'eat_pytorch_datasets/imdb/train_samples/'\n",
    "test_samples_path =  'eat_pytorch_datasets/imdb/test_samples/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8e126",
   "metadata": {},
   "source": [
    "首先我们构建词典，并保留最高频的MAX_WORDS个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "##构建词典\n",
    "\n",
    "word_count_dict = {}\n",
    "\n",
    "#清洗文本\n",
    "def clean_text(text):\n",
    "    lowercase = text.lower().replace(\"\\n\",\" \")\n",
    "    stripped_html = re.sub('<br />', ' ',lowercase)\n",
    "    cleaned_punctuation = re.sub('[%s]'%re.escape(string.punctuation),'',stripped_html)\n",
    "    return cleaned_punctuation\n",
    "\n",
    "with open(train_data_path,\"r\",encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "        label,text = line.split(\"\\t\")\n",
    "        cleaned_text = clean_text(text)\n",
    "        for word in cleaned_text.split(\" \"):\n",
    "            word_count_dict[word] = word_count_dict.get(word,0)+1 \n",
    "\n",
    "df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = \"count\"))\n",
    "df_word_dict = df_word_dict.sort_values(by = \"count\",ascending =False)\n",
    "\n",
    "df_word_dict = df_word_dict[0:MAX_WORDS-2] #  \n",
    "df_word_dict[\"word_id\"] = range(2,MAX_WORDS) #编号0和1分别留给未知词<unkown>和填充<padding>\n",
    "\n",
    "word_id_dict = df_word_dict[\"word_id\"].to_dict()\n",
    "\n",
    "df_word_dict.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf9482",
   "metadata": {},
   "source": [
    "![](./data/5-1-词典.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56422a1d",
   "metadata": {},
   "source": [
    "然后我们利用构建好的词典，将文本转换成token序号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换token\n",
    "\n",
    "# 填充文本\n",
    "def pad(data_list,pad_length):\n",
    "    padded_list = data_list.copy()\n",
    "    if len(data_list)> pad_length:\n",
    "         padded_list = data_list[-pad_length:]\n",
    "    if len(data_list)< pad_length:\n",
    "         padded_list = [1]*(pad_length-len(data_list))+data_list\n",
    "    return padded_list\n",
    "\n",
    "def text_to_token(text_file,token_file):\n",
    "    with open(text_file,\"r\",encoding = 'utf-8') as fin,\\\n",
    "      open(token_file,\"w\",encoding = 'utf-8') as fout:\n",
    "        for line in fin:\n",
    "            label,text = line.split(\"\\t\")\n",
    "            cleaned_text = clean_text(text)\n",
    "            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(\" \")]\n",
    "            pad_list = pad(word_token_list,MAX_LEN)\n",
    "            out_line = label+\"\\t\"+\" \".join([str(x) for x in pad_list])\n",
    "            fout.write(out_line+\"\\n\")\n",
    "        \n",
    "text_to_token(train_data_path,train_token_path)\n",
    "text_to_token(test_data_path,test_token_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7eb7f",
   "metadata": {},
   "source": [
    "接着将token文本按照样本分割，每个文件存放一个样本的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割样本\n",
    "import os\n",
    "\n",
    "if not os.path.exists(train_samples_path):\n",
    "    os.mkdir(train_samples_path)\n",
    "    \n",
    "if not os.path.exists(test_samples_path):\n",
    "    os.mkdir(test_samples_path)\n",
    "    \n",
    "    \n",
    "def split_samples(token_path,samples_dir):\n",
    "    with open(token_path,\"r\",encoding = 'utf-8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            with open(samples_dir+\"%d.txt\"%i,\"w\",encoding = \"utf-8\") as fout:\n",
    "                fout.write(line)\n",
    "            i = i+1\n",
    "\n",
    "split_samples(train_token_path,train_samples_path)\n",
    "split_samples(test_token_path,test_samples_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(train_samples_path)[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b47ba8",
   "metadata": {},
   "source": [
    "```\n",
    "['11303.txt', '3644.txt', '19987.txt', '18441.txt', '5235.txt', '17772.txt', '1053.txt', '13514.txt', '8711.txt', '15165.txt', '7422.txt', '8077.txt', '15603.txt', '7344.txt', '1735.txt', '13272.txt', '9369.txt', '18327.txt', '5553.txt', '17014.txt', '4895.txt', '11465.txt', '3122.txt', '19039.txt', '5547.txt', '18333.txt', '17000.txt', '4881.txt', '2228.txt', '11471.txt', '3136.txt', '4659.txt', '15617.txt', '8063.txt', '7350.txt', '12178.txt', '1721.txt', '13266.txt', '14509.txt', '6728.txt', '1047.txt', '13500.txt', '15171.txt', '8705.txt', '7436.txt', '16478.txt', '11317.txt', '3650.txt', '19993.txt', '10009.txt', '5221.txt', '18455.txt', '17766.txt', '3888.txt', '6700.txt', '14247.txt', '9433.txt', '13528.txt', '12636.txt', '15159.txt', '16450.txt', '4117.txt', '19763.txt', '3678.txt', '17996.txt', '2566.txt', '10021.txt', '5209.txt', '17028.txt', '2200.txt', '10747.txt', '11459.txt', '16336.txt', '4671.txt', '19005.txt', '7378.txt', '12150.txt', '1709.txt', '6066.txt', '14521.txt', '9355.txt', '12144.txt', '289.txt', '6072.txt', '9341.txt', '14535.txt', '2214.txt', '10753.txt', '16322.txt', '19011.txt', '4665.txt', '16444.txt', '19777.txt', '4103.txt', '17982.txt', '2572.txt', '10035.txt', '18469.txt', '6714.txt', '9427.txt']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5490ca0",
   "metadata": {},
   "source": [
    "一切准备就绪，我们可以创建数据集Dataset, 从文件名称列表中读取文件内容了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0643c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class imdbDataset(Dataset):\n",
    "    def __init__(self,samples_dir):\n",
    "        self.samples_dir = samples_dir\n",
    "        self.samples_paths = os.listdir(samples_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples_paths)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        path = self.samples_dir + self.samples_paths[index]\n",
    "        with open(path,\"r\",encoding = \"utf-8\") as f:\n",
    "            line = f.readline()\n",
    "            label,tokens = line.split(\"\\t\")\n",
    "            label = torch.tensor([float(label)],dtype = torch.float)\n",
    "            feature = torch.tensor([int(x) for x in tokens.split(\" \")],dtype = torch.long)\n",
    "            return  (feature,label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdefe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = imdbDataset(train_samples_path)\n",
    "ds_test = imdbDataset(test_samples_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7199043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds_train))\n",
    "print(len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0211ed",
   "metadata": {},
   "source": [
    "```\n",
    "20000\n",
    "5000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = True)\n",
    "dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE)\n",
    "\n",
    "for features,labels in dl_train:\n",
    "    break\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950faf1",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([[   1,    1,    1,  ...,   29,    8,    8],\n",
    "        [  13,   11,  247,  ...,    0,    0,    8],\n",
    "        [8587,  555,   12,  ...,    3,    0,    8],\n",
    "        ...,\n",
    "        [   1,    1,    1,  ...,    2,    0,    8],\n",
    "        [ 618,   62,   25,  ...,   20,  204,    8],\n",
    "        [   1,    1,    1,  ...,   71,   85,    8]])\n",
    "tensor([[1.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [1.],\n",
    "        [0.],\n",
    "        [1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333cdf4e",
   "metadata": {},
   "source": [
    "最后构建模型测试一下数据集管道是否可用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f3751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #设置padding_idx参数后将在训练过程中将填充的token始终赋值为0向量\n",
    "        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,embedding_dim = 3,padding_idx = 1)\n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\"conv_1\",nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))\n",
    "        self.conv.add_module(\"pool_1\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_1\",nn.ReLU())\n",
    "        self.conv.add_module(\"conv_2\",nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))\n",
    "        self.conv.add_module(\"pool_2\",nn.MaxPool1d(kernel_size = 2))\n",
    "        self.conv.add_module(\"relu_2\",nn.ReLU())\n",
    "        \n",
    "        self.dense = nn.Sequential()\n",
    "        self.dense.add_module(\"flatten\",nn.Flatten())\n",
    "        self.dense.add_module(\"linear\",nn.Linear(6144,1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.embedding(x).transpose(1,2)\n",
    "        x = self.conv(x)\n",
    "        y = self.dense(x)\n",
    "        return y\n",
    "        \n",
    "net = Net()\n",
    "\n",
    "\n",
    "preds = net(features)\n",
    "print(net)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdac8d8",
   "metadata": {},
   "source": [
    "```\n",
    "Net(\n",
    "  (embedding): Embedding(10000, 3, padding_idx=1)\n",
    "  (conv): Sequential(\n",
    "    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))\n",
    "    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (relu_1): ReLU()\n",
    "    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))\n",
    "    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "    (relu_2): ReLU()\n",
    "  )\n",
    "  (dense): Sequential(\n",
    "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
    "    (linear): Linear(in_features=6144, out_features=1, bias=True)\n",
    "  )\n",
    ")\n",
    "tensor([[-0.1576],\n",
    "        [-0.2841],\n",
    "        [-0.1405],\n",
    "        [-0.0690],\n",
    "        [-0.0520],\n",
    "        [-0.2680],\n",
    "        [-0.1689],\n",
    "        [-0.0814],\n",
    "        [-0.1718],\n",
    "        [-0.1264],\n",
    "        [-0.1674],\n",
    "        [-0.1763],\n",
    "        [-0.1537],\n",
    "        [-0.2502],\n",
    "        [-0.1983],\n",
    "        [-0.0943],\n",
    "        [-0.0087],\n",
    "        [-0.2154],\n",
    "        [-0.1263],\n",
    "        [-0.0274]], grad_fn=<AddmmBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11422d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras import KerasModel \n",
    "from torchkeras.metrics import Accuracy\n",
    "\n",
    "net = Net() \n",
    "model = KerasModel(net,\n",
    "                  loss_fn = nn.BCEWithLogitsLoss(),\n",
    "                  optimizer= torch.optim.Adam(net.parameters(),lr = 0.005),  \n",
    "                  metrics_dict = {\"acc\":Accuracy()}\n",
    "                )\n",
    "model.fit(dl_train,\n",
    "    val_data=dl_test,\n",
    "    epochs=10,\n",
    "    ckpt_path='checkpoint.pt',\n",
    "    patience=3,\n",
    "    monitor='val_acc',\n",
    "    mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6756b181",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================2022-07-17 15:13:03\n",
    "Epoch 4 / 10\n",
    "\n",
    "100%|██████████| 1000/1000 [00:16<00:00, 59.53it/s, train_acc=0.907, train_loss=0.237]\n",
    "100%|██████████| 250/250 [00:02<00:00, 107.14it/s, val_acc=0.87, val_loss=0.318] \n",
    "<<<<<< reach best val_acc : 0.8704000115394592 >>>>>>\n",
    "\n",
    "================================================================================2022-07-17 15:13:22\n",
    "Epoch 5 / 10\n",
    "\n",
    "100%|██████████| 1000/1000 [00:17<00:00, 58.38it/s, train_acc=0.928, train_loss=0.19]\n",
    "100%|██████████| 250/250 [00:02<00:00, 101.77it/s, val_acc=0.87, val_loss=0.338] \n",
    "\n",
    "================================================================================2022-07-17 15:13:42\n",
    "Epoch 6 / 10\n",
    "\n",
    "100%|██████████| 1000/1000 [00:16<00:00, 59.51it/s, train_acc=0.942, train_loss=0.153]\n",
    "100%|██████████| 250/250 [00:02<00:00, 104.84it/s, val_acc=0.856, val_loss=0.41] \n",
    "\n",
    "================================================================================2022-07-17 15:14:01\n",
    "Epoch 7 / 10\n",
    "\n",
    "100%|██████████| 1000/1000 [00:17<00:00, 58.78it/s, train_acc=0.954, train_loss=0.123]\n",
    "100%|██████████| 250/250 [00:02<00:00, 104.06it/s, val_acc=0.863, val_loss=0.428]\n",
    "<<<<<< val_acc without improvement in 3 epoch, early stopping >>>>>>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c72ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21007f11",
   "metadata": {},
   "source": [
    "### 三，使用DataLoader加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0cdca",
   "metadata": {},
   "source": [
    "DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。\n",
    "\n",
    "DataLoader的函数签名如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d6c0b",
   "metadata": {},
   "source": [
    "```python\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    multiprocessing_context=None,\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319544",
   "metadata": {},
   "source": [
    "一般情况下，我们仅仅会配置 dataset, batch_size, shuffle, num_workers, drop_last这五个参数，其他参数使用默认值即可。\n",
    "\n",
    "DataLoader除了可以加载我们前面讲的 torch.utils.data.Dataset 外，还能够加载另外一种数据集 torch.utils.data.IterableDataset。\n",
    "\n",
    "和Dataset数据集相当于一种列表结构不同，IterableDataset相当于一种迭代器结构。 它更加复杂，一般较少使用。\n",
    "\n",
    "- dataset : 数据集\n",
    "- batch_size: 批次大小\n",
    "- shuffle: 是否乱序\n",
    "- sampler: 样本采样函数，一般无需设置。\n",
    "- batch_sampler: 批次采样函数，一般无需设置。\n",
    "- num_workers: 使用多进程读取数据，设置的进程数。\n",
    "- collate_fn: 整理一个批次数据的函数。\n",
    "- pin_memory: 是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。\n",
    "- drop_last: 是否丢弃最后一个样本数量不足batch_size批次数据。\n",
    "- timeout: 加载一个数据批次的最长等待时间，一般无需设置。\n",
    "- worker_init_fn: 每个worker中dataset的初始化函数，常用于 IterableDataset。一般不使用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281047ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建输入数据管道\n",
    "ds = TensorDataset(torch.arange(1,50))\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = 10,\n",
    "                shuffle= True,\n",
    "                num_workers=2,\n",
    "                drop_last = True)\n",
    "#迭代数据\n",
    "for batch, in dl:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2856881",
   "metadata": {},
   "source": [
    "```\n",
    "tensor([43, 44, 21, 36,  9,  5, 28, 16, 20, 14])\n",
    "tensor([23, 49, 35, 38,  2, 34, 45, 18, 15, 40])\n",
    "tensor([26,  6, 27, 39,  8,  4, 24, 19, 32, 17])\n",
    "tensor([ 1, 29, 11, 47, 12, 22, 48, 42, 10,  7])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308db239",
   "metadata": {},
   "source": [
    "**如果本书对你有所帮助，想鼓励一下作者，记得给本项目加一颗星星star⭐️，并分享给你的朋友们喔😊!** \n",
    "\n",
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"算法美食屋\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "也可以在公众号后台回复关键字：**加群**，加入读者交流群和大家讨论。\n",
    "\n",
    "![算法美食屋logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9206fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
