{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5f5c64f",
   "metadata": {},
   "source": [
    "\n",
    "# 5-5,损失函数losses\n",
    "\n",
    "一般来说，监督学习的目标函数由损失函数和正则化项组成。(Objective = Loss + Regularization)\n",
    "\n",
    "Pytorch中的损失函数一般在训练模型时候指定。\n",
    "\n",
    "注意Pytorch中内置的损失函数的参数和tensorflow不同，是y_pred在前，y_true在后，而Tensorflow是y_true在前，y_pred在后。\n",
    "\n",
    "对于回归模型，通常使用的内置损失函数是均方损失函数nn.MSELoss 。\n",
    "\n",
    "对于二分类模型，通常使用的是二元交叉熵损失函数nn.BCELoss (输入已经是sigmoid激活函数之后的结果) \n",
    "或者 nn.BCEWithLogitsLoss (输入尚未经过nn.Sigmoid激活函数) 。\n",
    "\n",
    "对于多分类模型，一般推荐使用交叉熵损失函数 nn.CrossEntropyLoss。\n",
    "(y_true需要是一维的，是类别编码。y_pred未经过nn.Softmax激活。) \n",
    "\n",
    "此外，如果多分类的y_pred经过了nn.LogSoftmax激活，可以使用nn.NLLLoss损失函数(The negative log likelihood loss)。\n",
    "这种方法和直接使用nn.CrossEntropyLoss等价。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada0bd74",
   "metadata": {},
   "source": [
    "如果有需要，也可以自定义损失函数，自定义损失函数需要接收两个张量y_pred，y_true作为输入参数，并输出一个标量作为损失函数值。\n",
    "\n",
    "Pytorch中的正则化项一般通过自定义的方式和损失函数一起添加作为目标函数。\n",
    "\n",
    "如果仅仅使用L2正则化，也可以利用优化器的weight_decay参数来实现相同的效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchkeras\n",
    "\n",
    "print(\"torch.__version__=\"+torch.__version__) \n",
    "print(\"torchkeras.__version__=\"+torchkeras.__version__) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bdb60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b9a924b",
   "metadata": {},
   "source": [
    "```\n",
    "torch.__version__=1.10.0\n",
    "torchkeras.__version__=3.2.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0655388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "342d8b30",
   "metadata": {},
   "source": [
    "### 一，内置损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dea5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "from torch import nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "y_pred = torch.tensor([[10.0,0.0,-10.0],[8.0,8.0,8.0]])\n",
    "y_true = torch.tensor([0,2])\n",
    "\n",
    "# 直接调用交叉熵损失\n",
    "ce = nn.CrossEntropyLoss()(y_pred,y_true)\n",
    "print(ce)\n",
    "\n",
    "# 等价于先计算nn.LogSoftmax激活，再调用NLLLoss\n",
    "y_pred_logsoftmax = nn.LogSoftmax(dim = 1)(y_pred)\n",
    "nll = nn.NLLLoss()(y_pred_logsoftmax,y_true)\n",
    "print(nll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2342fdfb",
   "metadata": {},
   "source": [
    "```\n",
    "tensor(0.5493)\n",
    "tensor(0.5493)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ef7f1",
   "metadata": {},
   "source": [
    "内置的损失函数一般有类的实现和函数的实现两种形式。\n",
    "\n",
    "如：nn.BCE 和 F.binary_cross_entropy 都是二元交叉熵损失函数，前者是类的实现形式，后者是函数的实现形式。\n",
    "\n",
    "实际上类的实现形式通常是调用函数的实现形式并用nn.Module封装后得到的。\n",
    "\n",
    "一般我们常用的是类的实现形式。它们封装在torch.nn模块下，并且类名以Loss结尾。\n",
    "\n",
    "常用的一些内置损失函数说明如下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca231f8",
   "metadata": {},
   "source": [
    "* nn.MSELoss（均方误差损失，也叫做L2损失，用于回归）\n",
    "\n",
    "* nn.L1Loss （L1损失，也叫做绝对值误差损失，用于回归）\n",
    "\n",
    "* nn.SmoothL1Loss (平滑L1损失，当输入在-1到1之间时，平滑为L2损失，用于回归)\n",
    "\n",
    "* nn.BCELoss (二元交叉熵，用于二分类，输入已经过nn.Sigmoid激活，对不平衡数据集可以用weigths参数调整类别权重)\n",
    "\n",
    "* nn.BCEWithLogitsLoss (二元交叉熵，用于二分类，输入未经过nn.Sigmoid激活)\n",
    "\n",
    "* nn.CrossEntropyLoss (交叉熵，用于多分类，要求label为稀疏编码，输入未经过nn.Softmax激活，对不平衡数据集可以用weigths参数调整类别权重)\n",
    "\n",
    "* nn.NLLLoss (负对数似然损失，用于多分类，要求label为稀疏编码，输入经过nn.LogSoftmax激活)\n",
    "\n",
    "* nn.CosineSimilarity(余弦相似度，可用于多分类)\n",
    "\n",
    "* nn.AdaptiveLogSoftmaxWithLoss (一种适合非常多类别且类别分布很不均衡的损失函数，会自适应地将多个小类别合成一个cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd177c",
   "metadata": {},
   "source": [
    "更多损失函数的介绍参考如下知乎文章：\n",
    "\n",
    "《PyTorch的十八个损失函数》\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/61379965"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e226957",
   "metadata": {},
   "source": [
    "### 二，自定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d03399",
   "metadata": {},
   "source": [
    "自定义损失函数接收两个张量y_pred,y_true作为输入参数，并输出一个标量作为损失函数值。\n",
    "\n",
    "也可以对nn.Module进行子类化，重写forward方法实现损失的计算逻辑，从而得到损失函数的类的实现。\n",
    "\n",
    "下面是一个Focal Loss的自定义实现示范。Focal Loss是一种对binary_crossentropy的改进损失函数形式。\n",
    "\n",
    "它在样本不均衡和存在较多易分类的样本时相比binary_crossentropy具有明显的优势。\n",
    "\n",
    "它有两个可调参数，alpha参数和gamma参数。其中alpha参数主要用于衰减负样本的权重，gamma参数主要用于衰减容易训练样本的权重。\n",
    "\n",
    "从而让模型更加聚焦在正样本和困难样本上。这就是为什么这个损失函数叫做Focal Loss。\n",
    "\n",
    "详见《5分钟理解Focal Loss与GHM——解决样本不平衡利器》\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/80594704\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940e430",
   "metadata": {},
   "source": [
    "$$focal\\_loss(y,p) = \n",
    "\\begin{cases} -\\alpha (1-p)^{\\gamma}\\log(p) & \\text{if y = 1}\\\\\n",
    "-(1-\\alpha) p^{\\gamma}\\log(1-p) & \\text{if y = 0} \n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self,gamma=2.0,alpha=0.75):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self,y_pred,y_true):\n",
    "        bce = torch.nn.BCELoss(reduction = \"none\")(y_pred,y_true)\n",
    "        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))\n",
    "        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        modulating_factor = torch.pow(1.0 - p_t, self.gamma)\n",
    "        loss = torch.mean(alpha_factor * modulating_factor * bce)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#困难样本\n",
    "y_pred_hard = torch.tensor([[0.5],[0.5]])\n",
    "y_true_hard = torch.tensor([[1.0],[0.0]])\n",
    "\n",
    "#容易样本\n",
    "y_pred_easy = torch.tensor([[0.9],[0.1]])\n",
    "y_true_easy = torch.tensor([[1.0],[0.0]])\n",
    "\n",
    "focal_loss = FocalLoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "\n",
    "print(\"focal_loss(easy samples):\", focal_loss(y_pred_easy,y_true_easy))\n",
    "print(\"bce_loss(easy samples):\", bce_loss(y_pred_easy,y_true_easy))\n",
    "\n",
    "print(\"focal_loss(hard samples):\", focal_loss(y_pred_hard,y_true_hard))\n",
    "print(\"bce_loss(hard samples):\", bce_loss(y_pred_hard,y_true_hard))\n",
    "\n",
    "\n",
    "#可见 focal_loss让容易样本的权重衰减到原来的 0.0005/0.1054 = 0.00474\n",
    "#而让困难样本的权重只衰减到原来的 0.0866/0.6931=0.12496\n",
    "\n",
    "# 因此相对而言，focal_loss可以衰减容易样本的权重。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa888688",
   "metadata": {},
   "source": [
    "```\n",
    "focal_loss(easy samples): tensor(0.0005)\n",
    "bce_loss(easy samples): tensor(0.1054)\n",
    "focal_loss(hard samples): tensor(0.0866)\n",
    "bce_loss(hard samples): tensor(0.6931)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e66b5b",
   "metadata": {},
   "source": [
    "FocalLoss的使用完整范例可以参考下面中`自定义L1和L2正则化项`中的范例，该范例既演示了自定义正则化项的方法，也演示了FocalLoss的使用方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33cb90",
   "metadata": {},
   "source": [
    "### 三，自定义L1和L2正则化项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a4c74",
   "metadata": {},
   "source": [
    "通常认为L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择。\n",
    "\n",
    "而L2 正则化可以防止模型过拟合（overfitting）。一定程度上，L1也可以防止过拟合。\n",
    "\n",
    "下面以一个二分类问题为例，演示给模型的目标函数添加自定义L1和L2正则化项的方法。\n",
    "\n",
    "这个范例同时演示了上一个部分的FocalLoss的使用。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc27d8",
   "metadata": {},
   "source": [
    "**1，准备数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4685b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "import torchkeras \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "#正负样本数量\n",
    "n_positive,n_negative = 1000,6000\n",
    "\n",
    "#生成正样本, 小圆环分布\n",
    "r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) \n",
    "theta_p = 2*np.pi*torch.rand([n_positive,1])\n",
    "Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)\n",
    "Yp = torch.ones_like(r_p)\n",
    "\n",
    "#生成负样本, 大圆环分布\n",
    "r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) \n",
    "theta_n = 2*np.pi*torch.rand([n_negative,1])\n",
    "Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)\n",
    "Yn = torch.zeros_like(r_n)\n",
    "\n",
    "#汇总样本\n",
    "X = torch.cat([Xp,Xn],axis = 0)\n",
    "Y = torch.cat([Yp,Yn],axis = 0)\n",
    "\n",
    "\n",
    "#可视化\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.scatter(Xp[:,0],Xp[:,1],c = \"r\")\n",
    "plt.scatter(Xn[:,0],Xn[:,1],c = \"g\")\n",
    "plt.legend([\"positive\",\"negative\"]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a5ab5",
   "metadata": {},
   "source": [
    "![](./data/5-3-同心圆分布.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(X,Y)\n",
    "\n",
    "ds_train,ds_val = torch.utils.data.random_split(ds,[int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])\n",
    "dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True,num_workers=2)\n",
    "dl_val = DataLoader(ds_val,batch_size = 100,num_workers=2)\n",
    "\n",
    "features,labels = next(iter(dl_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7377e",
   "metadata": {},
   "source": [
    "**2，定义模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccaa5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2,4)\n",
    "        self.fc2 = nn.Linear(4,8) \n",
    "        self.fc3 = nn.Linear(8,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        y = self.fc3(x)\n",
    "        return y\n",
    "        \n",
    "net = Net() \n",
    "\n",
    "from torchkeras import summary\n",
    "\n",
    "summary(net,features);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820c615",
   "metadata": {},
   "source": [
    "```\n",
    "--------------------------------------------------------------------------\n",
    "Layer (type)                            Output Shape              Param #\n",
    "==========================================================================\n",
    "Linear-1                                     [-1, 4]                   12\n",
    "Linear-2                                     [-1, 8]                   40\n",
    "Linear-3                                     [-1, 1]                    9\n",
    "==========================================================================\n",
    "Total params: 61\n",
    "Trainable params: 61\n",
    "Non-trainable params: 0\n",
    "--------------------------------------------------------------------------\n",
    "Input size (MB): 0.000069\n",
    "Forward/backward pass size (MB): 0.000099\n",
    "Params size (MB): 0.000233\n",
    "Estimated Total Size (MB): 0.000401\n",
    "--------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b465f",
   "metadata": {},
   "source": [
    "**3，训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53233cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2正则化\n",
    "def L2Loss(model,alpha):\n",
    "    l2_loss = torch.tensor(0.0, requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name: #一般不对偏置项使用正则\n",
    "            l2_loss = l2_loss + (0.5 * alpha * torch.sum(torch.pow(param, 2)))\n",
    "    return l2_loss\n",
    "\n",
    "# L1正则化\n",
    "def L1Loss(model,beta):\n",
    "    l1_loss = torch.tensor(0.0, requires_grad=True)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            l1_loss = l1_loss +  beta * torch.sum(torch.abs(param))\n",
    "    return l1_loss\n",
    "\n",
    "# 将L2正则和L1正则添加到FocalLoss损失，一起作为目标函数\n",
    "def focal_loss_with_regularization(y_pred,y_true):\n",
    "    y_probs = torch.sigmoid(y_pred)\n",
    "    focal = FocalLoss()(y_probs,y_true) \n",
    "    l2_loss = L2Loss(model,0.0001) #注意设置正则化项系数\n",
    "    l1_loss = L1Loss(model,0.0001)\n",
    "    total_loss = focal + l2_loss + l1_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "from torchkeras import KerasModel\n",
    "from torchkeras.metrics import AUCROC \n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.002)\n",
    "model = KerasModel(net=net,\n",
    "                   loss_fn = focal_loss_with_regularization ,\n",
    "                   metrics_dict = {\"auc\":AUCROC()},\n",
    "                   optimizer= optimizer )\n",
    "\n",
    "\n",
    "dfhistory = model.fit(train_data=dl_train,\n",
    "      val_data=dl_val,\n",
    "      epochs=20,\n",
    "      ckpt_path='checkpoint.pt',\n",
    "      patience=3,\n",
    "      monitor='val_auc',\n",
    "      mode='max')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1233767",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================2022-07-17 15:59:24\n",
    "Epoch 15 / 20\n",
    "\n",
    "100%|██████████| 49/49 [00:02<00:00, 16.76it/s, train_auc=0.978, train_loss=0.0186]\n",
    "100%|██████████| 21/21 [00:02<00:00,  8.33it/s, val_auc=0.984, val_loss=0.0171]\n",
    "\n",
    "================================================================================2022-07-17 15:59:30\n",
    "Epoch 16 / 20\n",
    "\n",
    "100%|██████████| 49/49 [00:03<00:00, 14.31it/s, train_auc=0.978, train_loss=0.0183]\n",
    "100%|██████████| 21/21 [00:02<00:00,  9.09it/s, val_auc=0.985, val_loss=0.0166]\n",
    "<<<<<< reach best val_auc : 0.9853957295417786 >>>>>>\n",
    "\n",
    "================================================================================2022-07-17 15:59:35\n",
    "Epoch 17 / 20\n",
    "\n",
    "100%|██████████| 49/49 [00:02<00:00, 16.71it/s, train_auc=0.978, train_loss=0.018] \n",
    "100%|██████████| 21/21 [00:02<00:00,  9.37it/s, val_auc=0.986, val_loss=0.0161]\n",
    "<<<<<< reach best val_auc : 0.9860185384750366 >>>>>>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 结果可视化\n",
    "fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))\n",
    "ax1.scatter(Xp[:,0],Xp[:,1], c=\"r\")\n",
    "ax1.scatter(Xn[:,0],Xn[:,1],c = \"g\")\n",
    "ax1.legend([\"positive\",\"negative\"]);\n",
    "ax1.set_title(\"y_true\");\n",
    "\n",
    "Xp_pred = X[torch.squeeze(torch.sigmoid(net.forward(X))>=0.5)]\n",
    "Xn_pred = X[torch.squeeze(torch.sigmoid(net.forward(X))<0.5)]\n",
    "\n",
    "ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = \"r\")\n",
    "ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = \"g\")\n",
    "ax2.legend([\"positive\",\"negative\"]);\n",
    "ax2.set_title(\"y_pred\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fd191",
   "metadata": {},
   "source": [
    "![](./data/5-3-focal_loss预测结果.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593b1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a247f565",
   "metadata": {},
   "source": [
    "### 四，通过优化器实现L2正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69743b",
   "metadata": {},
   "source": [
    "如果仅仅需要使用L2正则化，那么也可以利用优化器的weight_decay参数来实现。\n",
    "\n",
    "weight_decay参数可以设置参数在训练过程中的衰减，这和L2正则化的作用效果等价。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4834eb60",
   "metadata": {},
   "source": [
    "```\n",
    "before L2 regularization:\n",
    "\n",
    "gradient descent: w = w - lr * dloss_dw \n",
    "\n",
    "after L2 regularization:\n",
    "\n",
    "gradient descent: w = w - lr * (dloss_dw+beta*w) = (1-lr*beta)*w - lr*dloss_dw\n",
    "\n",
    "so （1-lr*beta）is the weight decay ratio.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d16c8e",
   "metadata": {},
   "source": [
    "Pytorch的优化器支持一种称之为Per-parameter options的操作，就是对每一个参数进行特定的学习率，权重衰减率指定，以满足更为细致的要求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b677c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_params = [param for name, param in model.named_parameters() if \"bias\" not in name]\n",
    "bias_params = [param for name, param in model.named_parameters() if \"bias\" in name]\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params': weight_params, 'weight_decay':1e-5},\n",
    "                             {'params': bias_params, 'weight_decay':0}],\n",
    "                            lr=1e-2, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c63418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "399e51c3",
   "metadata": {},
   "source": [
    "**如果本书对你有所帮助，想鼓励一下作者，记得给本项目加一颗星星star⭐️，并分享给你的朋友们喔😊!** \n",
    "\n",
    "如果对本书内容理解上有需要进一步和作者交流的地方，欢迎在公众号\"算法美食屋\"下留言。作者时间和精力有限，会酌情予以回复。\n",
    "\n",
    "也可以在公众号后台回复关键字：**加群**，加入读者交流群和大家讨论。\n",
    "\n",
    "![算法美食屋logo.png](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e4719f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
